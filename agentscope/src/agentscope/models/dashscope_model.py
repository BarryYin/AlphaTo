# -*- coding: utf-8 -*-
"""Model wrapper for DashScope models"""
from abc import ABC
from http import HTTPStatus
from typing import Any, Union, List, Sequence
from loguru import logger

from ..message import Msg
from ..utils.tools import to_openai_dict, _convert_to_str

try:
    import dashscope
except ModuleNotFoundError:
    dashscope = None

from .model import ModelWrapperBase, ModelResponse

from ..file_manager import file_manager


class DashScopeWrapperBase(ModelWrapperBase, ABC):
    """The model wrapper for DashScope API."""

    def __init__(
        self,
        config_name: str,
        model_name: str = None,
        api_key: str = None,
        generate_args: dict = None,
        **kwargs: Any,
    ) -> None:
        """Initialize the DashScope wrapper.

        Args:
            config_name (`str`):
                The name of the model config.
            model_name (`str`, default `None`):
                The name of the model to use in DashScope API.
            api_key (`str`, default `None`):
                The API key for DashScope API.
            generate_args (`dict`, default `None`):
                The extra keyword arguments used in DashScope api generation,
                e.g. `temperature`, `seed`.
        """
        if model_name is None:
            model_name = config_name
            logger.warning("model_name is not set, use config_name instead.")

        super().__init__(config_name=config_name)

        if dashscope is None:
            raise ImportError(
                "Cannot find dashscope package in current python environment.",
            )

        self.model_name = model_name
        self.generate_args = generate_args or {}

        self.api_key = api_key
        dashscope.api_key = self.api_key
        self.max_length = None

        # Set monitor accordingly
        self._register_default_metrics()

    def format(
        self,
        *args: Union[Msg, Sequence[Msg]],
    ) -> Union[List[dict], str]:
        raise RuntimeError(
            f"Model Wrapper [{type(self).__name__}] doesn't "
            f"need to format the input. Please try to use the "
            f"model wrapper directly.",
        )


class DashScopeChatWrapper(DashScopeWrapperBase):
    """The model wrapper for DashScope's chat API."""

    model_type: str = "dashscope_chat"

    deprecated_model_type: str = "tongyi_chat"

    def _register_default_metrics(self) -> None:
        # Set monitor accordingly
        # TODO: set quota to the following metrics
        self.monitor.register(
            self._metric("call_counter"),
            metric_unit="times",
        )
        self.monitor.register(
            self._metric("prompt_tokens"),
            metric_unit="token",
        )
        self.monitor.register(
            self._metric("completion_tokens"),
            metric_unit="token",
        )
        self.monitor.register(
            self._metric("total_tokens"),
            metric_unit="token",
        )

    def __call__(
        self,
        messages: list,
        **kwargs: Any,
    ) -> ModelResponse:
        """Processes a list of messages to construct a payload for the
        DashScope API call. It then makes a request to the DashScope API
        and returns the response. This method also updates monitoring
        metrics based on the API response.

        Each message in the 'messages' list can contain text content and
        optionally an 'image_urls' key. If 'image_urls' is provided,
        it is expected to be a list of strings representing URLs to images.
        These URLs will be transformed to a suitable format for the DashScope
        API, which might involve converting local file paths to data URIs.

        Args:
            messages (`list`):
                A list of messages to process.
            **kwargs (`Any`):
                The keyword arguments to DashScope chat completions API,
                e.g. `temperature`, `max_tokens`, `top_p`, etc. Please
                refer to
                https://help.aliyun.com/zh/dashscope/developer-reference/api-details
                for more detailed arguments.

        Returns:
            `ModelResponse`:
                The response text in text field, and the raw response in
                raw field.

        Note:
            `parse_func`, `fault_handler` and `max_retries` are reserved for
            `_response_parse_decorator` to parse and check the response
            generated by model wrapper. Their usages are listed as follows:
                - `parse_func` is a callable function used to parse and check
                the response generated by the model, which takes the response
                as input.
                - `max_retries` is the maximum number of retries when the
                `parse_func` raise an exception.
                - `fault_handler` is a callable function which is called
                when the response generated by the model is invalid after
                `max_retries` retries.
            The rule of roles in messages for DashScope is very rigid,
            for more details, please refer to
            https://help.aliyun.com/zh/dashscope/developer-reference/api-details
        """

        # step1: prepare keyword arguments
        kwargs = {**self.generate_args, **kwargs}

        # step2: checking messages
        if not isinstance(messages, list):
            raise ValueError(
                "Dashscope `messages` field expected type `list`, "
                f"got `{type(messages)}` instead.",
            )
        if not all("role" in msg and "content" in msg for msg in messages):
            raise ValueError(
                "Each message in the 'messages' list must contain a 'role' "
                "and 'content' key for DashScope API.",
            )

        # TODO: move is to prompt engineering
        messages = self._preprocess_role(messages)
        # step3: forward to generate response
        response = dashscope.Generation.call(
            model=self.model_name,
            messages=messages,
            result_format="message",  # set the result to be "message" format.
            **kwargs,
        )

        if response.status_code != HTTPStatus.OK:
            error_msg = (
                f" Request id: {response.request_id},"
                f" Status code: {response.status_code},"
                f" error code: {response.code},"
                f" error message: {response.message}."
            )

            raise RuntimeError(error_msg)

        # step4: record the api invocation if needed
        self._save_model_invocation(
            arguments={
                "model": self.model_name,
                "messages": messages,
                **kwargs,
            },
            response=response,
        )

        # step5: update monitor accordingly
        # The metric names are unified for comparison
        self.update_monitor(
            call_counter=1,
            prompt_tokens=response.usage["input_tokens"],
            completion_tokens=response.usage["output_tokens"],
            total_tokens=response.usage["total_tokens"],
        )

        # step6: return response
        return ModelResponse(
            text=response.output["choices"][0]["message"]["content"],
            raw=response,
        )

    def format(
        self,
        *args: Union[Msg, Sequence[Msg]],
    ) -> List:
        """Format the messages for DashScope Chat API.

        In this format function, the input messages are converted into
        dictionaries with `role` and `content` fields. This conversation may
        not meet the requirement that `user` and `assistant` speak
        alternatively. This requirement can be enforced by calling
        `preprocess_role` function..

        # TODO: We will merge these two functions into one `format` function
        soon.

        The following is an example:

        .. code-block:: python

            prompt = model.format(
                Msg("system", "You're a helpful assistant", role="system"),
                Msg("Bob", "Hi, how can I help you?", role="assistant"),
                Msg("user", "What's the date today?", role="user")
            )

        The prompt will be as follows:

        .. code-block:: python

            [
                {"role": "system", "content": "You are a helpful assistant"},
                {"role": "assistant", "content": "Hi, how can I help you"},
                {"role": "assistant", "content": "What's the date today?"}
            ]


        Args:
            *args (`Union[Msg, Sequence[Msg]]`):
                The input arguments to be formatted, where each argument
                should be a `Msg` object, or a list of `Msg` objects

        Returns:
            `List[dict]`:
                The formatted messages.
        """
        # TODO: This function only convert agentscope msgs into qwen
        #  messages, the re-range is executed in _preprocess_role function.

        # TODO: This strategy will be replaced by a new strategy in the future.
        prompt = []
        for unit in args:
            if unit is None:
                continue
            if isinstance(unit, Msg):
                prompt.append(to_openai_dict(unit))
            elif isinstance(unit, list):
                for child_unit in unit:
                    if isinstance(child_unit, Msg):
                        prompt.append(to_openai_dict(child_unit))
                    else:
                        raise TypeError(
                            f"The input should be a Msg object or a list "
                            f"of Msg objects, got {type(child_unit)}.",
                        )
            else:
                raise TypeError(
                    f"The input should be a Msg object or a list "
                    f"of Msg objects, got {type(unit)}.",
                )

        return prompt

    def advanced_format(
        self,
        *args: Union[Msg, Sequence[Msg]],
    ) -> List:
        """Format the messages for DashScope Chat API.

        In this format function, the input messages are formatted into a
        single system messages with format "{name}: {content}" for each
        message. Note this strategy maybe not suitable for all scenarios,
        and developers are encouraged to implement their own prompt
        engineering strategies.

        The following is an example:

        .. code-block:: python

            prompt = model.format(
                Msg("system", "You're a helpful assistant", role="system"),
                Msg("Bob", "Hi, how can I help you?", role="assistant"),
                Msg("user", "What's the date today?", role="user")
            )

        The prompt will be as follows:

        .. code-block:: python

            [
                {
                    "role": "system",
                    "content": (
                       "system: You're a helpful assistant\n",
                       "Bob: Hi, how can I help you?\n",
                       "user: What's the date today?"
                    )
                }
            ]


        Args:
            *args (`Union[Msg, Sequence[Msg]]`):
                The input arguments to be formatted, where each argument
                should be a `Msg` object, or a list of `Msg` objects

        Returns:
            `List[dict]`:
                The formatted messages.
        """
        # TODO: This function only convert agentscope msgs into qwen
        #  messages, the re-range is executed in _preprocess_role function.
        prompt = []
        for unit in args:
            if isinstance(unit, Msg):
                prompt.append(f"{unit.name}: {_convert_to_str(unit.content)}")
            elif isinstance(unit, list):
                for child_unit in unit:
                    if isinstance(child_unit, Msg):
                        prompt.append(
                            f"{child_unit.name}: "
                            f"{_convert_to_str(child_unit.content)}",
                        )
                    else:
                        raise TypeError(
                            f"The input should be a Msg object or a list "
                            f"of Msg objects, got {type(child_unit)}.",
                        )
            else:
                raise TypeError(
                    f"The input should be a Msg object or a list "
                    f"of Msg objects, got {type(unit)}.",
                )

        prompt_str = "\n".join(prompt)

        return [{"role": "system", "content": prompt_str}]

    def _preprocess_role(self, messages: list) -> list:
        """preprocess role rules for DashScope"""
        # The models in this list require that the roles of messages must
        # alternate between "user" and "assistant".
        message_length = len(messages)
        if message_length % 2 == 1:
            # If the length of the message list is odd, roles will
            # alternate, starting with "user"
            roles = [
                "user" if i % 2 == 0 else "assistant"
                for i in range(message_length)
            ]
        else:
            # If the length of the message list is even, the first role
            # will be "system", followed by alternating "user" and
            # "assistant"
            roles = ["system"] + [
                "user" if i % 2 == 1 else "assistant"
                for i in range(1, message_length)
            ]

        # Assign the roles list to the "role" key for each message in
        # the messages list
        for message, role in zip(messages, roles):
            message["role"] = role

        return messages


class DashScopeImageSynthesisWrapper(DashScopeWrapperBase):
    """The model wrapper for DashScope Image Synthesis API."""

    model_type: str = "dashscope_image_synthesis"

    def _register_default_metrics(self) -> None:
        # Set monitor accordingly
        # TODO: set quota to the following metrics
        self.monitor.register(
            self._metric("call_counter"),
            metric_unit="times",
        )
        self.monitor.register(
            self._metric("image_count"),
            metric_unit="image",
        )

    def __call__(
        self,
        prompt: str,
        save_local: bool = False,
        **kwargs: Any,
    ) -> ModelResponse:
        """
         Args:
             prompt (`str`):
                 The prompt string to generate images from.
             save_local: (`bool`, default `False`):
                 Whether to save the generated images locally, and replace
                 the returned image url with the local path.
             **kwargs (`Any`):
                 The keyword arguments to DashScope Image Synthesis API,
                 e.g. `n`, `size`, etc. Please refer to
                 https://help.aliyun.com/zh/dashscope/developer-reference/api-details-9
        for more detailed arguments.

         Returns:
             `ModelResponse`:
                 A list of image urls in image_urls field and the
                 raw response in raw field.

         Note:
             `parse_func`, `fault_handler` and `max_retries` are reserved
             for `_response_parse_decorator` to parse and check the
             response generated by model wrapper. Their usages are listed
             as follows:
                 - `parse_func` is a callable function used to parse and
                 check the response generated by the model, which takes
                 the response as input.
                 - `max_retries` is the maximum number of retries when the
                 `parse_func` raise an exception.
                 - `fault_handler` is a callable function which is called
                 when the response generated by the model is invalid after
                 `max_retries` retries.
        """
        # step1: prepare keyword arguments
        kwargs = {**self.generate_args, **kwargs}

        # step2: forward to generate response
        response = dashscope.ImageSynthesis.call(
            model=self.model_name,
            prompt=prompt,
            n=1,
            **kwargs,
        )
        if response.status_code != HTTPStatus.OK:
            error_msg = (
                f" Request id: {response.request_id},"
                f" Status code: {response.status_code},"
                f" error code: {response.code},"
                f" error message: {response.message}."
            )
            raise RuntimeError(error_msg)

        # step3: record the model api invocation if needed
        self._save_model_invocation(
            arguments={
                "model": self.model_name,
                "prompt": prompt,
                **kwargs,
            },
            response=response,
        )

        # step4: update monitor accordingly
        self.update_monitor(
            call_counter=1,
            **response.usage,
        )

        # step5: return response
        images = response.output["results"]
        # Get image urls as a list
        urls = [_["url"] for _ in images]

        if save_local:
            # Return local url if save_local is True
            urls = [file_manager.save_image(_) for _ in urls]
        return ModelResponse(image_urls=urls, raw=response)


class DashScopeTextEmbeddingWrapper(DashScopeWrapperBase):
    """The model wrapper for DashScope Text Embedding API."""

    model_type: str = "dashscope_text_embedding"

    def _register_default_metrics(self) -> None:
        # Set monitor accordingly
        # TODO: set quota to the following metrics
        self.monitor.register(
            self._metric("call_counter"),
            metric_unit="times",
        )
        self.monitor.register(
            self._metric("total_tokens"),
            metric_unit="token",
        )

    def __call__(
        self,
        texts: Union[list[str], str],
        **kwargs: Any,
    ) -> ModelResponse:
        """Embed the messages with DashScope Text Embedding API.

        Args:
            texts (`list[str]` or `str`):
                The messages used to embed.
            **kwargs (`Any`):
                The keyword arguments to DashScope Text Embedding API,
                e.g. `text_type`. Please refer to
                https://help.aliyun.com/zh/dashscope/developer-reference/api-details-15
                for more detailed arguments.

        Returns:
            `ModelResponse`:
                A list of embeddings in embedding field and the raw
                response in raw field.

        Note:
            `parse_func`, `fault_handler` and `max_retries` are reserved
            for `_response_parse_decorator` to parse and check the response
            generated by model wrapper. Their usages are listed as follows:
                - `parse_func` is a callable function used to parse and
                check the response generated by the model, which takes the
                response as input.
                - `max_retries` is the maximum number of retries when the
                `parse_func` raise an exception.
                - `fault_handler` is a callable function which is called
                when the response generated by the model is invalid after
                `max_retries` retries.
        """
        # step1: prepare keyword arguments
        kwargs = {**self.generate_args, **kwargs}

        # step2: forward to generate response
        response = dashscope.TextEmbedding.call(
            input=texts,
            model=self.model_name,
            **kwargs,
        )

        if response.status_code != HTTPStatus.OK:
            error_msg = (
                f" Request id: {response.request_id},"
                f" Status code: {response.status_code},"
                f" error code: {response.code},"
                f" error message: {response.message}."
            )
            raise RuntimeError(error_msg)

        # step3: record the model api invocation if needed
        self._save_model_invocation(
            arguments={
                "model": self.model_name,
                "input": texts,
                **kwargs,
            },
            response=response,
        )

        # step4: update monitor accordingly
        self.update_monitor(
            call_counter=1,
            **response.usage,
        )

        # step5: return response
        if len(response.output["embeddings"]) == 0:
            return ModelResponse(
                embedding=response.output["embedding"][0],
                raw=response,
            )
        else:
            return ModelResponse(
                embedding=[
                    _["embedding"] for _ in response.output["embeddings"]
                ],
                raw=response,
            )
